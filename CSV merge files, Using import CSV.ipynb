{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"I:\\UTCS\\Region\\SAmerica.Hou\\Expl_Proj\\South_America\\Users\\srorta\\WellPressureSnowflake3PDO_IHS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\UTCS\\Region\\SAmerica.Hou\\Expl_Proj\\South_America\\Users\\srorta\\WellPressureSnowflake3PDO_IHS\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "texas = glob.glob(os.path.join(path,\"TEXAS_*.csv\"))\n",
    "kansas = glob.glob(os.path.join(path,\"KANSAS*.csv\"))\n",
    "newmex = glob.glob(os.path.join(path,\"NEW_MEXICO*.csv\"))\n",
    "oklahoma = glob.glob(os.path.join(path,\"OKLAHOMA*.csv\"))\n",
    "louisiana = glob.glob(os.path.join(path,\"LOUISIANA*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_1.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_10.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_2.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_3.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_4.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_5.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_6.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_7.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_8.csv',\n",
       " 'I:\\\\UTCS\\\\Region\\\\SAmerica.Hou\\\\Expl_Proj\\\\South_America\\\\Users\\\\srorta\\\\WellPressureSnowflake3PDO_IHS\\\\TEXAS_9.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿ASSIGNED_FIELD', 'BASE_DEPTH_METRIC', 'BASE_DEPTH_METRIC_UOM', 'BASE_DEPTH_REPORTED', 'BASE_DEPTH_REPORTED_UOM', 'BASE_DEPTH_USCUST', 'BASE_DEPTH_USCUST_UOM', 'BASE_STRAT_AGE', 'BASE_STRAT_UNIT', 'BASE_STRAT_UNIT_ID', 'BASIN', 'COORDINATING_OPERATOR', 'COUNTRY', 'COUNTY', 'DATA_SOURCE', 'FILE_NAME', 'FLOW_CASING_PRESS_METRIC', 'FLOW_CASING_PRESS_METRIC_UOM', 'FLOW_CASING_PRESS_REPORTED', 'FLOW_CASING_PRESS_REPORTED_UOM', 'FLOW_CASING_PRESS_USCUST', 'FLOW_CASING_PRESS_USCUST_UOM', 'FLOW_TUBING_PRESS_METRIC', 'FLOW_TUBING_PRESS_METRIC_UOM', 'FLOW_TUBING_PRESS_REPORTED', 'FLOW_TUBING_PRESS_REPORTED_UOM', 'FLOW_TUBING_PRESS_USCUST', 'FLOW_TUBING_PRESS_USCUST_UOM', 'FLUID_TYPE', 'INITIAL_RESERVOIR_PRESS_METRIC', 'INITIAL_RESERVOIR_PRESS_METRIC_UOM', 'INITIAL_RESERVOIR_PRESS_REPORTED', 'INITIAL_RESERVOIR_PRESS_REPORTED_UOM', 'INITIAL_RESERVOIR_PRESS_USCUST', 'INITIAL_RESERVOIR_PRESS_USCUST_UOM', 'INSERTED_DATE', 'PLAY', 'PLAY_TYPE', 'POOL', 'POOL_DATUM_DEPTH_METRIC', 'POOL_DATUM_DEPTH_METRIC_UOM', 'POOL_DATUM_DEPTH_REPORTED', 'POOL_DATUM_DEPTH_REPORTED_UOM', 'POOL_DATUM_DEPTH_USCUST', 'POOL_DATUM_DEPTH_USCUST_UOM', 'PRESS_OBS_NO', 'PRIMARY_STRAT_AGE', 'PRIMARY_STRAT_UNIT', 'PRIMARY_STRAT_UNIT_ID', 'PROD_STRING_FORMATION_OBS_NO', 'PRODUCT_KEY', 'PROVINCE_STATE', 'REMARK', 'ROW_ID', 'ROW_QUALITY', 'SHUTIN_CASING_PRESS_METRIC', 'SHUTIN_CASING_PRESS_METRIC_UOM', 'SHUTIN_CASING_PRESS_REPORTED', 'SHUTIN_CASING_PRESS_REPORTED_UOM', 'SHUTIN_CASING_PRESS_USCUST', 'SHUTIN_CASING_PRESS_USCUST_UOM', 'SHUTIN_TUBING_PRESS_METRIC', 'SHUTIN_TUBING_PRESS_METRIC_UOM', 'SHUTIN_TUBING_PRESS_REPORTED', 'SHUTIN_TUBING_PRESS_REPORTED_UOM', 'SHUTIN_TUBING_PRESS_USCUST', 'SHUTIN_TUBING_PRESS_USCUST_UOM', 'SOURCE', 'SUB_BASIN', 'SUB_PLAY', 'TEST_END_DATE', 'TEST_START_DATE', 'TOP_DEPTH_METRIC', 'TOP_DEPTH_METRIC_UOM', 'TOP_DEPTH_REPORTED', 'TOP_DEPTH_REPORTED_UOM', 'TOP_DEPTH_USCUST', 'TOP_DEPTH_USCUST_UOM', 'TOP_STRAT_AGE', 'TOP_STRAT_UNIT', 'TOP_STRAT_UNIT_ID', 'UPDATED_DATE', 'UWI', 'WELL_DATUM_DEPTH_METRIC', 'WELL_DATUM_DEPTH_METRIC_UOM', 'WELL_DATUM_DEPTH_REPORTED', 'WELL_DATUM_DEPTH_REPORTED_UOM', 'WELL_DATUM_DEPTH_USCUST', 'WELL_DATUM_DEPTH_USCUST_UOM', 'WELL_TEST_NUM', 'WELL_TEST_RUN_NUM', 'WELL_TEST_SOURCE', 'WELL_TEST_TYPE', 'WKT', 'GEOGRAPHY']\n"
     ]
    }
   ],
   "source": [
    "fieldnames = []\n",
    "for filename in texas:\n",
    "    with open(filename, \"r\", newline=\"\") as f_in:\n",
    "        reader = csv.reader(f_in)\n",
    "        headers = next(reader)\n",
    "        for h in headers:\n",
    "            if h not in fieldnames:\n",
    "                fieldnames.append(h)\n",
    "\n",
    "print(fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('API_NUMBER', 'Unnamed: 0_level_1'), ('A_PICKETT', 'Unnamed: 1_level_1'), ('COMPANY', 'Unnamed: 2_level_1'), ('COUNTRY', 'Unnamed: 3_level_1'), ('COUNTY', 'Unnamed: 4_level_1'), ('DATE_LOGGED', 'Unnamed: 5_level_1'), ('DATUM_ELEVATION', 'Unnamed: 6_level_1'), ('EDF', 'FEET'), ('EGL', 'FEET'), ('EKB', 'FEET'), ('EPD', 'FEET'), ('FIELD', 'Unnamed: 11_level_1'), ('LATITUDE', 'DEGREES'), ('LOCATION', 'Unnamed: 13_level_1'), ('LONGITUDE', 'DEGREES'), ('M_PICKETT', 'Unnamed: 15_level_1'), ('N_PICKETT', 'Unnamed: 16_level_1'), ('OPERATOR', 'Unnamed: 17_level_1'), ('RANG', 'Unnamed: 18_level_1'), ('RW_PICKETT', 'OHMM'), ('SECT', 'Unnamed: 20_level_1'), ('STATE', 'Unnamed: 21_level_1'), ('SURFACE_ELEV', 'FEET'), ('TOWN', 'Unnamed: 23_level_1'), ('UNIQUE_WELL_ID', 'Unnamed: 24_level_1'), ('WELL_NAME', 'Unnamed: 25_level_1')]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 27 fields in line 7, saw 39\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f4647a342910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mGeolog_headers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srorta\\appdata\\local\\conda\\conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srorta\\appdata\\local\\conda\\conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srorta\\appdata\\local\\conda\\conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\srorta\\appdata\\local\\conda\\conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2157\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2158\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 27 fields in line 7, saw 39\n"
     ]
    }
   ],
   "source": [
    "#test trying to use pandas, not used\n",
    "Geolog_headers = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, header = [0,1])\n",
    "    col = df.columns.values.tolist()\n",
    "    print(col)\n",
    "    for c in col:\n",
    "        if c not in Geolog_headers:\n",
    "            Geolog_headers.append(c)\n",
    "print(Geolog_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then copy the data\n",
    "with open(r\"I:\\UTCS\\Region\\SAmerica.Hou\\Expl_Proj\\South_America\\Users\\srorta\\WellPressureSnowflake3PDO_IHS\\TEXAS.csv\", \"w\", newline=\"\") as f_out:   # Comment 2 below\n",
    "  writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "  writer.writeheader()\n",
    "  for filename in texas:\n",
    "    with open(filename, \"r\", newline=\"\") as f_in:\n",
    "      reader = csv.DictReader(f_in)  # Uses the field names in this file\n",
    "      for line in reader:\n",
    "        # Comment 3 below\n",
    "        writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
